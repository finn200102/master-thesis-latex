
\section{Maschine Learning}
\label{sec:maschinelearning}
\subsection{Regression}
Für Regressionsaufgaben wird zwischen einer Antwortvariable \(Y\) und einer erklärenden Varaible \(X\) bzw. mehreren ein Zusammenhang hergestellt mit einem Model dieser Daten. Dieses Model wird durch Parameter spezifizierte, diese zu schätzen ist Aufgabe der Regression. Um den Fehler des Models festzustellen wird die Methode der kleinsten Quadrate verwendet, dabei werden die Parameter so gewählt, dass diese die 'residuar sum of squares' (RSS) minimieren. Neben der RSS existieren weiter Fehlermaße die von Nutze sind, bei der Bewertung von Regressionsmodeelen.

\subsubsection{Mean Squared Error (MSE)}
Für die Optimierung von den meißten Regressionsmodellen ist eine Penalisierung von größeren Abweichungen gewünscht, um so schnell an das Optimum navigieren. Dies ermöglicht der Mean Squared Error (MSE), dieser entspricht der RSS dividiert durch die Anzahl der Beobachtungen und ist somit der durchschnittliche quadrierte Fehler:
\begin{equation}
  \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \frac{RSS}{n}
  \label{eq:mse}
\end{equation}
Hierbei \(y_i\) die beobachteten Werte, \(\hat{y}_i\) die vorhergesagten Werte und \(n\) die Anzahl der Beobachtungen darstellt.

\subsubsection{Root Mean Squared Error (RMSE)}
Der MSE lässt sich aufgrund der quadratischen Form nicht intuitiv interpretieren, dies lösst der Root Mean Squared Error (RMSE). Der RMSE ist die Quadratwurzel des MSE und kann so in der selben Einheit wie die Zielvariable interpretiert werden:

\begin{equation}
  \text{RMSE} = \sqrt{\text{MSE}} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}
  \label{eq:rmse}
\end{equation}

Diese Eigenschaft macht den RMSE wertvoll für die praktische Interpretation der Modellgüte, da die durchschnittliche Abweichungen zwischen Vorhersage und den tatsächlichen Werten direkt angegeben wird. 

\subsubsection{Mean Absolute Error}
Die Quadratisierung der Werte für zu einem besondren Fokuss auf die großen, stark abweichenden Werte, diese eigenet sich für die mathematische Optimierung und für Datensätze ohne extreme Werte. Für Datensätze mit solchen extremen Werten eignet sich der Mean Absolute Error (MAE). Dieser verwendet die absoluten Abweichungen und ist so weniger sensitiv auf Ausreißer:

\begin{equation}
  \text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
  \label{eq:mae}
\end{equation}

\subsubsection{Mean Absolute Precentage Error (MAPE)}

Fehlermaße die eine Einheit tragen, lassen sich schlecht untereinander zwischen verschiedenen Datensätzen vergleichen, da die Zielvariablen im Allgemeinen in verschiedenen Größen vorliegen werden. Der Mean Absolute Precentage Error (MAPE) drückt den durchschnittliche absoluten Fehler als Prozentsatz aus und ermöglicht dadurch eine einheitenlose Bewertung der Modellgüte.

\begin{equation}
\text{MAPE} = \frac{1}{n} \sum_{i=1}^{n} \left|\frac{y_i - \hat{y}_i}{y_i}\right| \times 100\%
\label{eq:mape}
\end{equation}

\subsection{Lineareregression}
Die Lineareregression ist ein Modell aus der Statistik welches den Zusammenhang zwischen einer Antwortvariable \(Y\) und einer erklärenden Varaible \(X\) herstellt. Das verwendete Modell ist ein lineares Modell es wird also die Annahme getroffen, dass zwischen \(Y\) und \(X\) ein linearer Zusammenhang vorliegt. Für eine einzelne erklärende Variable \(X\) ergeben sich zwei unbekannte Konstanten \(\beta_0\) und \(\beta_1\), diese zubestimmen ist das Ziel der linearen Regression. Das Modell lässt sich somit schreiben als:
\begin{equation}
    Y \simeq \beta_0 + \beta_1 X
    \label{eq:linreg1}
\end{equation}
Es ist zu unterscheiden zwischen den Modell Koeffizienten \(\beta\) und den Schätzungen dieser Koeffizienten \(\hat{\beta}\), die sich aus den verwendeten Trainingsdaten ergeben. 
Für das lineare Modell lässt sich ein direkter Ausdruck für die geschätzen Werte bilden, ausgedrückt über den Mittelwert von \(Y\) und \(X\):
\begin{equation}
    \hat{\beta_1} = \frac{\sum_{i=1}^{n}(x_i - \overline{x})(y_i - \overline{y})}{\sum_{i=1}^{n}(x_i - \overline{x})^2}
\end{equation}
\begin{equation}
    \hat{\beta_0} = \overline{y} - \hat{\beta_1}\overline{x}
\end{equation}

\cite{james2013}

\subsection{Refularisierte Lineareregression}

\subsubsection{Ridge Regression (L2-Regularisierung)}
Wenn in dem Vektor der erklärenden Variablen X viele miteinander korrelierte Werte vorliegen, dann kann die L2-Regularisierung verwendet werden, diese führt dazu dass korrelierte Variablen ähnliche Koeffizienten erhalten. Diese Methode ist Ridge Regression, also die Kombination aus Lineareregression und einem weiteren L2-Term. Die Kostenfuktion, hier der RSS, wird um einen weitren Strafterm erweitert.

\begin{equation}
\text{RSS}_{\text{Ridge}} = \sum_{i=1}^{n}(y_i - \beta_0 - \sum_{j=1}^{p}\beta_j x_{ij})^2 + \lambda \sum_{j=1}^{p}\beta_j^2
\label{eq:rss-ridge}
\end{equation}
Der Hyperparameter \(\lambda \geq 0\) kontrolliert die Stärke der L2-Regularisierung. Bei \(\lambda = 0\) entspricht die Ridge-Regression der Lineareregression. Größere Werte von \(\lambda \) reduzieren die Größe der Koeffizienten.

\subsubsection{Lasso Regression}
Die Lineareregression eignet sich nicht für hoch dimensionale Datensätze. Wenn viele dieser weiteren Werte irrelevant für die Regression sind, dann kann eine L1-Regularisierung verwendet werden, da diese Koeffizienten auf null setzen kann. Diese Variante der Lineareregression ist die Lasso Regression (Least Absolute Shrinkage and Selection Operator). Sowohl die L1 als auch die L2 Norm, führen neben diesen Eigenschaften zu einer Regularisierung und reduzieren beide so overfitting.

\begin{equation}
\text{RSS}_{\text{Lasso}} = \sum_{i=1}^{n}(y_i - \beta_0 - \sum_{j=1}^{p}\beta_j x_{ij})^2 + \lambda \sum_{j=1}^{p}|\beta_j|
\label{eq:reg-lasso}
\end{equation}

\subsubsection{Elastic Net}
Die Vorteile der Ridge und der Lasso Regression lassen sich kombinieren, durch die Verwendung beider Regressionsterme.

\begin{equation}
\text{RSS}_{\text{Elastic Net}} = \sum_{i=1}^{n}(y_i - \beta_0 - \sum_{j=1}^{p}\beta_j x_{ij})^2 + \lambda_1 \sum_{j=1}^{p}|\beta_j| + \lambda_2 \sum_{j=1}^{p}\beta_j^2
\label{eq:reg-elastic}
\end{equation}

\subsection{Baumbasierte Verfahren}


% Clustering Verfahren kmeans, hdbscan, pca
% Features (statisticle, fft)
% deeplearning (cnn, lstm, pinn)
% Dimensionsreduktioin (PCA, UMAP, t-SNE)
